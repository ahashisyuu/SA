{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def load_rawData(data_path):\n",
    "    trainingset = None\n",
    "    validationset = None\n",
    "    testa = None\n",
    "    listdir = os.listdir(data_path)\n",
    "    for name in listdir:\n",
    "        if 'trainingset.csv' in name:\n",
    "            trainingset = pd.read_csv(os.path.join(data_path, name))\n",
    "        if 'validationset.csv' in name:\n",
    "            validationset = pd.read_csv(os.path.join(data_path, name))\n",
    "        if 'testa.csv' in name:\n",
    "            testa = pd.read_csv(os.path.join(data_path, name))\n",
    "    return trainingset, validationset, testa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset, validationset, testa = load_rawData('../rawData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对有标签的数据进行处理\n",
    "trainingset.iloc[:, 2:] = trainingset.iloc[:, 2:].apply(lambda x: x+2)\n",
    "validationset.iloc[:, 2:] = validationset.iloc[:, 2:].apply(lambda x: x+2)\n",
    "# 标签onehot化\n",
    "def one_hot(number):\n",
    "    li = [0, 0, 0, 0]\n",
    "    li[number] = 1\n",
    "    return li\n",
    "\n",
    "def one_hot_series(series):\n",
    "    return series.apply(one_hot)\n",
    "\n",
    "\n",
    "trainingset.iloc[:, 2:] = trainingset.iloc[:, 2:].apply(one_hot_series)\n",
    "validationset.iloc[:, 2:] = validationset.iloc[:, 2:].apply(one_hot_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def tokenize(content, filters='！!“”\"#$%&（）()*+,，-。、./:：；;‘’《》……·<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    return [token for token in jieba.cut(content[1:-1]) if token not in filters]\n",
    "\n",
    "trainingset['content'] = trainingset['content'].apply(tokenize)\n",
    "validationset['content'] = validationset['content'].apply(tokenize)\n",
    "testa['content'] = testa['content'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_path = '../data'\n",
    "\n",
    "trainingset.to_csv(os.path.join(new_data_path, 'trainingset.csv'))\n",
    "validationset.to_csv(os.path.join(new_data_path, 'validationset.csv'))\n",
    "testa.to_csv(os.path.join(new_data_path, 'testa.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
